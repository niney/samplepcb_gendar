# Configuration for RoBERTa-base model training

model:
  name: roberta-base
  num_labels: 3
  dropout: 0.1

training:
  num_epochs: 12
  batch_size: 16
  learning_rate: 1e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  
  optimizer: adamw
  scheduler: linear
  
  gradient_accumulation_steps: 2
  max_grad_norm: 1.0
  
  fp16: true
  
  # Early stopping
  early_stopping_patience: 3

data:
  max_length: 512
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15

logging:
  log_dir: logs
  save_steps: 500
  eval_steps: 500
  logging_steps: 100
